{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Deep Learning - Master in Deep Learning of UPM</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejercicio práctico de hoy utilizaremos como ejemplo para aplicar shallow embeddings el ya famoso _toy dataset_ \"KarateClub\" donde un club de Karate tras un conflicto, se separó en dos \"Mr. Hi\" y \"Officer\". Cada nodo hace referencia a un miembro y cada arista a si dos miembros han interactuado fuera del club. Nuestro objetivo será utilizar shallow embeddings para obtener representaciones de cada uno de los nodos para tratar de predecir el club de cierto miembro dada sus relaciones fuera del club con el resto de miembros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.patches as mpatches\n",
    "from IPython.display import HTML\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils import to_networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NO TOCAR*\n",
    "\n",
    "Esta clase nos servirá para visualizaciones tanto estáticas como dinámicas del grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphVisualizer:\n",
    "    def __init__(self, G: nx.Graph, cmap='Pastel1', seed=42):\n",
    "        self.G = G\n",
    "        self.cmap = cmap\n",
    "        self.fig, self.ax = plt.subplots(figsize=(6, 6))\n",
    "        self.pos = nx.spring_layout(G, seed=42)\n",
    "        self.node_colors = self._generate_node_colors()\n",
    "        self.seed = seed\n",
    "\n",
    "    def _generate_node_colors(self):\n",
    "        \"\"\"Genera colores para los nodos basados en sus etiquetas.\"\"\"\n",
    "        if not hasattr(self.G, 'labels') or self.G.labels is None:\n",
    "            self.label_to_color = {node: '#A1D6CB' for node in self.G.nodes}\n",
    "            return ['#A1D6CB'] * len(self.G.nodes)\n",
    "        else:\n",
    "            # Extraemos las etiquetas de los nodos\n",
    "            labels = self.G.labels.values() # Asume que el grafo tiene un atributo 'labels' con las etiquetas de los nodos\n",
    "\n",
    "            # Creamos una lista de colores para las etiquetas\n",
    "            color_map = plt.colormaps[self.cmap]  # Colores automáticos\n",
    "            unique_labels, num_labels = set(labels), len(set(labels))\n",
    "            self.label_to_color = {label: color_map(i / num_labels) for i, label in enumerate(unique_labels)}\n",
    "\n",
    "            # Extraemos los colores para cada nodo\n",
    "            return [self.label_to_color[node] for node in labels]\n",
    "    \n",
    "    def _random_walk_strategy(self, neighbors: List[int], walk: List[int] = None, **kwargs) -> int:\n",
    "        selected = random.choice(neighbors) # Elegimos un vecino aleatorio\n",
    "        return selected, None # Devolvemos el vecino seleccionado\n",
    "    \n",
    "    def _node2vec_walk_strategy(self, neighbors: List[int], walk: List[int] = None, p=1, q=1, **kwargs):\n",
    "        \"\"\"\n",
    "        p: Return parameter - Controla la probabilidad de volver al nodo anterior, valores altos(>1) reducen la probabilidad de regresar.\n",
    "        q: In-out parameter - Controla la probabilidad de visitar nodos no visitados recientemente, valores altos(>1) favorecen la exploración.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(walk) == 1:\n",
    "            return random.choice(neighbors), None\n",
    "\n",
    "        prev_node = walk[-2] # Nodo previo (penúltimo nodo visitado)\n",
    "        weights = []\n",
    "\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor == prev_node: # El vecino es el nodo previo\n",
    "                weights.append(1 / p)\n",
    "            elif self.G.has_edge(prev_node, neighbor): # El vecino es vecino directo del nodo previo\n",
    "                weights.append(1)\n",
    "            else: # El vecino es un vecino lejano\n",
    "                weights.append(1 / q)\n",
    "\n",
    "        # Normalizamos las probabilidades\n",
    "        total = sum(weights)\n",
    "        weights = [prob / total for prob in weights]\n",
    "\n",
    "        # Elegimos el siguiente nodo en función de las probabilidades\n",
    "        selected = random.choices(neighbors, weights=weights, k=1)[0]\n",
    "\n",
    "        return selected, weights\n",
    "\n",
    "    def _init_animation(self):\n",
    "        \"\"\"Inicializa la figura para la animación.\"\"\"\n",
    "        self.ax.clear()\n",
    "        nx.draw(self.G, self.pos, ax=self.ax, with_labels=True, node_color=self.node_colors, font_size=8, edge_color=\"gray\")\n",
    "        return self.ax,\n",
    "\n",
    "    def _update_animation(self, frame):\n",
    "        \"\"\"Actualiza la animación en cada frame.\"\"\"\n",
    "        self.ax.clear()\n",
    "        nx.draw(self.G, self.pos, ax=self.ax, with_labels=True, node_color=self.node_colors, font_size=8, edge_color=\"gray\")\n",
    "\n",
    "        step = frame // 2\n",
    "        current_node = self.walk[step]\n",
    "        visited_nodes = self.walk[:step + 1]\n",
    "        visited_edges = [(self.walk[i], self.walk[i + 1]) for i in range(len(visited_nodes) - 1)]\n",
    "\n",
    "        # Dibujar nodos y aristas visitados\n",
    "        nx.draw_networkx_nodes(self.G, self.pos, ax=self.ax, nodelist=visited_nodes, node_color=\"yellow\")\n",
    "        nx.draw_networkx_edges(self.G, self.pos, ax=self.ax, edgelist=visited_edges, edge_color=\"red\", width=2, alpha=0.3)\n",
    "\n",
    "        # Destacar el nodo actual\n",
    "        nx.draw_networkx_nodes(self.G, self.pos, ax=self.ax, nodelist=[current_node], node_color=\"green\")\n",
    "\n",
    "        neighbors_info = \"\"\n",
    "        if frame % 2 == 1: # Expansión\n",
    "            neighbors = list(nx.all_neighbors(self.G, self.node))\n",
    "            neighbors_edges = [(self.node, neighbor) for neighbor in neighbors]\n",
    "            selected, weights = self.R(neighbors, walk=self.walk, **self.R_params)\n",
    "            self.node = selected\n",
    "            self.walk.append(self.node)\n",
    "            nx.draw_networkx_edges(self.G, self.pos, ax=self.ax, edgelist=neighbors_edges, edge_color=\"blue\", width=2, alpha=0.3)\n",
    "\n",
    "            if weights:\n",
    "                neighbors_weights = {neighbor: weight for neighbor, weight in zip(neighbors, weights)}\n",
    "            else:\n",
    "                neighbors_weights = {neighbor: 1./len(neighbors) for neighbor in neighbors}\n",
    "            neighbors_info = \"\\n\".join([f\"Node {neighbor}: {weight:.2f}\" for neighbor, weight in neighbors_weights.items()])\n",
    "\n",
    "        self.ax.text(0.01, 0.99, f'Start Node: {self.start_node}\\nStep: {step + 1}\\nCurrent Walk: {\", \".join(map(str, visited_nodes))}\\n{neighbors_info}',\n",
    "                     transform=self.ax.transAxes, fontsize=9, verticalalignment='top',\n",
    "                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "\n",
    "        return self.ax,\n",
    "\n",
    "    def visualize(self, edge_labels=None, legend=True):\n",
    "        # Dibujamos los nodos y las aristas\n",
    "        nx.draw_networkx_nodes(self.G, ax=self.ax, pos=self.pos, node_color=self.node_colors, node_size=500)\n",
    "        nx.draw_networkx_edges(self.G, ax=self.ax, pos=self.pos, alpha=0.3)\n",
    "        \n",
    "        # Dibujamos las etiquetas de los nodos (IDs)\n",
    "        nx.draw_networkx_labels(self.G, self.pos, labels={node: node for node in self.G.nodes()}, font_size=8)\n",
    "\n",
    "        # Dibujamos las etiquetas de las aristas\n",
    "        if edge_labels:\n",
    "            nx.draw_networkx_edge_labels(self.G, self.pos, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "        # Creamos la leyenda\n",
    "        if legend:\n",
    "            legend_patches = [mpatches.Patch(color=color, label=label) for label, color in self.label_to_color.items()]\n",
    "            self.ax.legend(handles=legend_patches, loc='best')\n",
    " \n",
    "        plt.show()\n",
    "\n",
    "    def visualize_walk(self,\n",
    "                        R, # Función R o estrategia de camino \n",
    "                        start_node: int, # Nodo de inicio\n",
    "                        walk_length: int, # Longitud del camino\n",
    "                        save_path: str = None, # Ruta donde guardar la animación\n",
    "                        **kwargs: Dict[str, Any]\n",
    "                    ) -> FuncAnimation:\n",
    "        \"\"\"Crea y retorna la animación de un camino dada cierta estrategia.\"\"\"\n",
    "        match R:\n",
    "            case 'random_walk':\n",
    "                self.R = self._random_walk_strategy\n",
    "            case 'node2vec':\n",
    "                self.R = self._node2vec_walk_strategy\n",
    "        self.R_params = kwargs\n",
    "        self.start_node = start_node\n",
    "        self.walk_length = walk_length\n",
    "        self.walk = [start_node]\n",
    "        self.node = start_node\n",
    "\n",
    "        \"\"\"Crea y retorna la animación del random walk.\"\"\"\n",
    "        ani = FuncAnimation(self.fig, self._update_animation, frames=2 * self.walk_length, init_func=self._init_animation, blit=False, interval=1000)\n",
    "        \n",
    "        if save_path:\n",
    "            # Guarda la animación como un GIF\n",
    "            ani.save(save_path, writer='pillow')\n",
    "\n",
    "        plt.close(self.fig)\n",
    "        return ani\n",
    "    \n",
    "    def visualize_graph_with_pca(self, features):\n",
    "        embeddings = features.drop(columns=['label']) # Eliminamos la columna de etiquetas\n",
    "        pca = PCA(n_components=2, random_state=42)\n",
    "        coords = pca.fit_transform(embeddings)\n",
    "\n",
    "        pos = {int(node): (coords[i, 0], coords[i, 1]) for i, node in enumerate(features.index)}\n",
    "        \n",
    "        # Dibujar el grafo con las coordenadas PCA\n",
    "        nx.draw_networkx_nodes(self.G, pos, node_color=self.node_colors, cmap=\"Set2\", node_size=500)\n",
    "        nx.draw_networkx_edges(self.G, pos, alpha=0.3)\n",
    "        \n",
    "        # Dibujar las etiquetas de los nodos\n",
    "        nx.draw_networkx_labels(self.G, pos, labels={node: node for node in self.G.nodes()}, font_size=8)\n",
    "        \n",
    "        plt.title(\"Visualización del grafo con PCA\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grafos con NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.karate_club_graph() # Cargamos el grafo de la red social de un club de karate desde el propio NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in G.nodes(data=True):\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El _grafo_ de networkX es un objeto de Python al cual podemos agregarle atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Características de cada nodo (solo la etiqueta en este caso)\n",
    "features = list(G.nodes()[0].keys())\n",
    "G.num_features = len(features)\n",
    "# Número de clases (en este caso, dos: 'Mr. Hi' y 'Officer', haciendo referencia al club de pertenencia)\n",
    "classes = set(nx.get_node_attributes(G, 'club').values())\n",
    "G.num_classes = len(classes)\n",
    "# Guardemos la etiqueta de cada nodo en una lista\n",
    "G.labels = {node[0]: node[1]['club'] for node in G.nodes(data=True)}\n",
    "\n",
    "print('Propiedades del grafo')\n",
    "print('==============================================================')\n",
    "print(f'Dataset: {G}') # Nombre del dataset\n",
    "print(f'Características: {features}') # Features de cada nodo\n",
    "print(f'Clases: {classes}') # Clases posibles\n",
    "print(f'Grado medio: {G.number_of_edges() / G.number_of_nodes():.2f}') # Average number of nodes in the graph\n",
    "print(f'Es dirigido: {G.is_directed()}') #Is the graph an undirected graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos el grafo con las etiquetas de los nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = GraphVisualizer(G)\n",
    "visualizer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Walk realiza caminos puramente aleatorios de manera que se pueda obtener una representación no sesgada de la localización del nodo dentro del grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función R (Estrategia de camino)\n",
    "def random_walk_strategy(G: nx.Graph, node: int) -> int:\n",
    "    neighbors = ... # Extraemos los vecinos del nodo actual\n",
    "    selected = ... # Elegimos un vecino aleatorio\n",
    "    return selected # Devolvemos el vecino seleccionado\n",
    "\n",
    "# Función de RandomWalk dado un nodo\n",
    "def random_walk_from_node(G: nx.Graph, start_node: int, walk_length: int) -> List[int]:\n",
    "    walk = ... # Inicializamos la lista de nodos visitados con el inicial\n",
    "    node = start_node # Puntero para el nodo actual\n",
    "\n",
    "    for _ in range(walk_length - 1):\n",
    "        selected = random_walk_strategy(G, node) # Elegimos un vecino aleatorio\n",
    "        ... # Añadimos el vecino a la lista de nodos visitados\n",
    "        ... # Actualizamos el puntero al nodo actual\n",
    "    \n",
    "    return walk\n",
    "\n",
    "start_node = 31\n",
    "walk_length = 5\n",
    "\n",
    "walk = random_walk_from_node(G, start_node=start_node, walk_length=walk_length)\n",
    "\n",
    "print(f'Random walk: {walk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualicemos como se ve la estrategia _R_ RandomWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_node = 31\n",
    "walk_length = 10\n",
    "visualizer = GraphVisualizer(G)\n",
    "ani = visualizer.visualize_walk(R='random_walk', start_node=start_node, walk_length=walk_length)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función random_walk() nos dará un número dado de secuencias de una longitud dada desde cada uno de los nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(G, walk_length=10, num_walks_per_node=10):\n",
    "    walks = []\n",
    "\n",
    "    for node in G.nodes():\n",
    "        for _ in range(num_walks_per_node): # Obtenemos varios random walks por nodo\n",
    "            walks.append(random_walk_from_node(G, node, walk_length=walk_length))\n",
    "    \n",
    "    return walks\n",
    "\n",
    "random_walks = random_walk(G, walk_length=10, num_walks_per_node=2)\n",
    "print(f'Primeras 5 random walks: {random_walks[:5]}')\n",
    "print(f'Últimas 5 random walks: {random_walks[-5:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node2Vec realiza también caminos aleatorios pero los pondera en función de dos parámetros:\n",
    "- p: Return parameter - Controla la probabilidad de volver al nodo anterior, valores altos(>1) reducen la probabilidad de regresar.\n",
    "- q: In-out parameter - Controla la probabilidad de visitar nodos no visitados recientemente, valores altos(>1) favorecen la exploración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2vec_walk_strategy(G: nx.Graph, node: int, walk: List[int] = None, p=1, q=1):\n",
    "    \"\"\"\n",
    "    p: Return parameter - Controla la probabilidad de volver al nodo anterior, valores altos(>1) reducen la probabilidad de regresar.\n",
    "    q: In-out parameter - Controla la probabilidad de visitar nodos no visitados recientemente, valores altos(>1) favorecen la exploración.\n",
    "    \"\"\"\n",
    "    neighbors = list(G.neighbors(node)) # Extraemos los vecinos del nodo actual\n",
    "\n",
    "    if len(walk) == 1:\n",
    "        selected = ... # Elegimos un vecino aleatorio\n",
    "        weights = None\n",
    "        return selected, weights\n",
    "\n",
    "    prev_node = ... # Nodo previo (penúltimo nodo visitado)\n",
    "    weights = []\n",
    "\n",
    "    for neighbor in neighbors:\n",
    "        if ... # El vecino es el nodo previo\n",
    "            ...\n",
    "        elif ... # El vecino es vecino directo del nodo previo\n",
    "            ...\n",
    "        else: # El vecino es un vecino lejano\n",
    "            ...\n",
    "\n",
    "    # Normalizamos las probabilidades\n",
    "    total = sum(weights)\n",
    "    weights = [prob / total for prob in weights]\n",
    "\n",
    "    # Elegimos el siguiente nodo en función de las probabilidades\n",
    "    selected = random.choices(neighbors, weights=weights, k=1)[0]\n",
    "\n",
    "    return selected, weights\n",
    "    \n",
    "\n",
    "def node2vec_walk_from_node(G: nx.Graph, start_node: int, walk_length: int, p: float, q: float) -> List[int]:\n",
    "    walk = [start_node] # Inicializamos la lista de nodos visitados con el inicial\n",
    "    node = start_node # Puntero para el nodo actual\n",
    "\n",
    "    for _ in range(walk_length - 1):\n",
    "        selected, _ = node2vec_walk_strategy(G, node, walk=walk, p=p, q=q)\n",
    "        walk.append(selected) # Añadimos el vecino a la lista de nodos visitados\n",
    "        node = selected\n",
    "\n",
    "    return walk\n",
    "\n",
    "start_node = 31\n",
    "walk_length = 10\n",
    "p = 2 \n",
    "q = 0.5\n",
    "\n",
    "walk = node2vec_walk_from_node(G, start_node=start_node, walk_length=walk_length, p=p, q=q)\n",
    "print(f'Node2Vec walk: {walk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualicemos como se ve la estrategia _R_ Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_node = 11\n",
    "walk_length = 10\n",
    "p = 2\n",
    "q = 0.5\n",
    "\n",
    "visualizer = GraphVisualizer(G)\n",
    "ani = visualizer.visualize_walk(R='node2vec', start_node=start_node, walk_length=walk_length, p=p, q=q)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función, al igual que random_walk(), nos dará cierto número de secuencias de cierta longitud dada pero con la estrategia _R_ propia de Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2vec(G, walk_length=10, num_walks_per_node=10, p=1, q=1):\n",
    "    walks = []\n",
    "\n",
    "    for node in G.nodes():\n",
    "        for _ in range(num_walks_per_node): # Obtenemos varios random walks por nodo\n",
    "            walks.append(node2vec_walk_from_node(G, node, walk_length=walk_length, p=p, q=q))\n",
    "    \n",
    "    return walks\n",
    "\n",
    "\n",
    "walk_length = 10\n",
    "num_walks_per_node = 2\n",
    "p = 2\n",
    "q = 0.5\n",
    "\n",
    "node2vec_walks = node2vec(G, walk_length=walk_length, num_walks_per_node=num_walks_per_node, p=p, q=q)\n",
    "print(f'Primeras 5 Node2Vec walks: {node2vec_walks[:5]}')\n",
    "print(f'Últimas 5 Node2Vec walks: {node2vec_walks[-5:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec para obtener representaciones de los caminos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec nos permite obtener representaciones para ciertos elementos de un vocabulario dado. Cada representación viene dada por la aparición de una _palabra_ (o _nodo_ en este caso) en el contexto de _frases_ (o _caminos_).\n",
    "\n",
    "El objetivo a la hora de entrenar el modelo (ya implementado en _gensim_) será generalmente predecir cierto nodo enmascarado solo teniendo acceso al resto de la secuencia. De esta forma, en el _embedding_ de cada nodo, se refleja la información estructural dados los caminos con los que entrenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_length = 10\n",
    "num_walks_per_node = 1\n",
    "p = 2\n",
    "q = 0.5\n",
    "\n",
    "# walks = random_walk(G, walk_length=walk_length, num_walks_per_node=num_walks_per_node)\n",
    "walks = node2vec(G, walk_length=walk_length, num_walks_per_node=num_walks_per_node, p=p, q=q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec necesita que los elementos (palabras) sean _strings_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = [[str(i) for i in walk] for walk in walks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos el modelo Word2Vec\n",
    "embedder = Word2Vec(\n",
    "   vector_size = 100, # Tamaño del vector de embeddings\n",
    "   window = 4, # Tamaño de la ventana\n",
    "   sg = 1, # Usamos Skip-Gram\n",
    "   hs = 0, # Usamos Negative Sampling\n",
    "   negative = 10, # Número de muestras negativas\n",
    "   alpha = 0.03, # Tasa de aprendizaje\n",
    "   min_alpha = 0.0001, # Tasa de aprendizaje mínima\n",
    "   min_count = 1, # Número mínimo de veces que una palabra debe aparecer en el corpus\n",
    "   seed = 42 # Semilla para reproducibilidad\n",
    ")\n",
    "# Construimos el vocabulario\n",
    "embedder.build_vocab(\n",
    "   corpus_iterable=walks, # Corpus de random walks\n",
    "   progress_per=2, # Cada cuántas iteraciones mostrar el progreso\n",
    ")\n",
    "\n",
    "# Entrenamos el modelo\n",
    "embedder.train(\n",
    "   corpus_iterable=walks, # Corpus de random walks\n",
    "   total_examples=embedder.corpus_count, # Número total de ejemplos\n",
    "   epochs=20, # Número de épocas\n",
    "   report_delay=1 # Cada cuántas épocas mostrar el progreso\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(embedder.wv.key_to_index.keys())\n",
    "\n",
    "assert len(vocabulary) == G.number_of_nodes(), \"El tamaño del vocabulario no coincide con el número de nodos del grafo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta clase será un _wrapper_ de la de Word2Vec de _gensim_ que simplificará y adaptará el uso de esta a nuestra tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEncoder:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.encoder = Word2Vec(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, walks, epochs: int=20, progress_per: int=2):\n",
    "        if not isinstance(walks[0][0], str):\n",
    "            walks = [[str(i) for i in walk] for walk in walks]\n",
    "\n",
    "        self.encoder.build_vocab(walks, progress_per=progress_per)\n",
    "        self.encoder.train(walks, total_examples=self.encoder.corpus_count, epochs=epochs, report_delay=1)\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not isinstance(X[0], str):\n",
    "            X = [str(i) for i in X]\n",
    "        \n",
    "        return {i: self.encoder.wv[i] for i in X if i in self.encoder.wv}\n",
    "\n",
    "    def fit_transform(self, walks, epochs=20, progress_per=2):\n",
    "        self.fit(walks, epochs=epochs, progress_per=progress_per)\n",
    "        X = list(self.encoder.wv.key_to_index.keys())\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = NodeEncoder(\n",
    "    vector_size = 10, # Tamaño del vector de embeddings\n",
    "    window = 4, # Tamaño de la ventana\n",
    "    sg = 1, # Usamos Skip-Gram\n",
    "    hs = 0, # Usamos Negative Sampling\n",
    "    negative = 10, # Número de muestras negativas\n",
    "    alpha = 0.03, # Tasa de aprendizaje\n",
    "    min_alpha = 0.0001, # Tasa de aprendizaje mínima\n",
    "    min_count = 1, # Número mínimo de veces que una palabra debe aparecer en el corpus\n",
    "    seed = 42 # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "embeddings = encoder.fit_transform(walks, epochs=20, progress_per=2) # Generamos los embeddings\n",
    "\n",
    "embeddings = pd.DataFrame(embeddings).T # Mostramos los primeros embeddings en forma de DataFrame\n",
    "embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapearemos las etiquetas de cada nodo a un valor entero para poder entrenar un clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2label = {'Mr. Hi': 0, 'Officer': 1} # Mapeo de etiquetas a enteros\n",
    "labels = {str(idx): [str2label[label]] for idx, label in G.labels.items()} # Obtenemos las etiquetas de los nodos\n",
    "\n",
    "labels = pd.DataFrame(labels).T # Mostramos las etiquetas en forma de DataFrame\n",
    "labels.columns = ['label']\n",
    "\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, combinaremos los _embeddings_ (o _features_) y las etiquetas teniendo como resultado un caso de \"clasificación de datos tabulares\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(embeddings, labels, left_index=True, right_index=True) # Unimos los embeddings y las etiquetas\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(df, eval_size=0.2, random_state=42):\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1] # Separamos las características y las etiquetas\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=eval_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_eval, y_train, y_eval\n",
    "\n",
    "def train_classifier(X_train, y_train, X_eval=None, y_eval=None, classifier=None):\n",
    "    if classifier is None:\n",
    "        classifier = RandomForestClassifier() # Inicializamos el clasificador\n",
    "    classifier = classifier.fit(X_train, y_train) # Entrenamos el clasificador\n",
    "\n",
    "    if X_eval is not None and y_eval is not None:\n",
    "        preds = classifier.predict(X_eval) # Predecimos las etiquetas de entrenamiento\n",
    "        acc = accuracy_score(y_eval, preds)\n",
    "        f1 = f1_score(y_eval, preds)\n",
    "        print(f'Accuracy: {acc:.2f} - F1 Score: {f1:.2f}')\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = generate_dataset(df, eval_size=0.2) # Generamos el dataset\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, # Número de árboles\n",
    "    max_depth=5, # Profundidad máxima\n",
    "    random_state=42 # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "classifier = train_classifier(X_train, y_train, X_eval, y_eval) # Entrenamos el clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de experimentación será obtener un rendimiento decente (f1 y acc > 0.85) con la única restricción de que el tamaño de camino máximo que podemos usar es de 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HIPERPARÁMETROS ###\n",
    "\n",
    "# Estrategia de camino\n",
    "walk_length = 5\n",
    "num_walks_per_node = ...\n",
    "p = ...\n",
    "q = ...\n",
    "\n",
    "# Encoder\n",
    "vector_size = ... # Tamaño del vector de embeddings\n",
    "window = 4 # Tamaño de la ventana\n",
    "sg = 1 # Usamos Skip-Gram\n",
    "hs = 0 # Usamos Negative Sampling\n",
    "negative = 10 # Número de muestras negativas\n",
    "alpha = 0.03 # Tasa de aprendizaje\n",
    "min_alpha = 0.0001 # Tasa de aprendizaje mínima\n",
    "min_count = 1 # Número mínimo de veces que una palabra debe aparecer en el corpus\n",
    "seed = 42 # Semilla para reproducibilidad\n",
    "\n",
    "epochs = 20\n",
    "progress_per = 2\n",
    "\n",
    "# Clasificador\n",
    "eval_size = 0.6\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=..., # Número de árboles\n",
    "    max_depth=..., # Profundidad máxima\n",
    "    random_state=seed # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Obtenemos los caminos\n",
    "# walks = random_walk(G, walk_length=walk_length, num_walks_per_node=num_walks_per_node)\n",
    "walks = node2vec(G, walk_length=walk_length, num_walks_per_node=num_walks_per_node, p=p, q=q)\n",
    "\n",
    "encoder = NodeEncoder(\n",
    "    vector_size = vector_size, # Tamaño del vector de embeddings\n",
    "    window = window, # Tamaño de la ventana\n",
    "    sg = sg, # Usamos Skip-Gram\n",
    "    hs = hs, # Usamos Negative Sampling\n",
    "    negative = negative, # Número de muestras negativas\n",
    "    alpha = alpha, # Tasa de aprendizaje\n",
    "    min_alpha = min_alpha, # Tasa de aprendizaje mínima\n",
    "    min_count = min_count, # Número mínimo de veces que una palabra debe aparecer en el corpus\n",
    "    seed = seed # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Codificamos los nodos\n",
    "embeddings = encoder.fit_transform(walks, epochs=epochs, progress_per=progress_per) # Generamos los embeddings\n",
    "embeddings = pd.DataFrame(embeddings).T # Mostramos los primeros embeddings en forma de DataFrame\n",
    "\n",
    "str2label = {'Mr. Hi': 0, 'Officer': 1} # Mapeo de etiquetas a enteros\n",
    "labels = {str(idx): [str2label[label]] for idx, label in G.labels.items()} # Obtenemos las etiquetas de los nodos\n",
    "\n",
    "labels = pd.DataFrame(labels).T # Mostramos las etiquetas en forma de DataFrame\n",
    "labels.columns = ['label']\n",
    "\n",
    "df = pd.merge(embeddings, labels, left_index=True, right_index=True) # Unimos los embeddings y las etiquetas\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = generate_dataset(df, eval_size=eval_size) # Generamos el dataset\n",
    "classifier = train_classifier(X_train, y_train, X_eval, y_eval, classifier=classifier) # Entrenamos el clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función reduce la dimensión obtenida de los embeddings mediante PCA a 2 dimensiones de manera que se pueden entender como coordenadas (x, y) y ser ploteadas en función de esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = GraphVisualizer(G)\n",
    "visualizer.visualize_graph_with_pca(features=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings para aristas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la predicción de aristas vamos a tomar el mismo dataset y transformarlo de manera que la etiqueta de la arista que conecta dos nodos será:\n",
    "- 0: si los dos miembros eran de clubs diferentes\n",
    "- 1: si los dos miembros pertenecían al mismo club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las etiquetas de los nodos (clubes)\n",
    "node_clubs = nx.get_node_attributes(G, \"club\")\n",
    "\n",
    "# Etiquetar las aristas: 1 si los nodos pertenecen al mismo club, 0 si no\n",
    "edge_labels = {}\n",
    "for u, v in G.edges():\n",
    "    edge_labels[(u, v)] = 1 if node_clubs[u] == node_clubs[v] else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualicemos como están etiquetadas las aristas en función de nuestro criterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = GraphVisualizer(G)\n",
    "visualizer.visualize(edge_labels=edge_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtengamos de la misma manera los embeddings para los nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HIPERPARÁMETROS ###\n",
    "\n",
    "# Estrategia de camino\n",
    "walk_length = 5\n",
    "num_walks_per_node = 5\n",
    "p = 1\n",
    "q = 1\n",
    "\n",
    "# Encoder\n",
    "vector_size = 10 # Tamaño del vector de embeddings\n",
    "window = 4 # Tamaño de la ventana\n",
    "sg = 1 # Usamos Skip-Gram\n",
    "hs = 0 # Usamos Negative Sampling\n",
    "negative = 10 # Número de muestras negativas\n",
    "alpha = 0.03 # Tasa de aprendizaje\n",
    "min_alpha = 0.0001 # Tasa de aprendizaje mínima\n",
    "min_count = 1 # Número mínimo de veces que una palabra debe aparecer en el corpus\n",
    "seed = 42 # Semilla para reproducibilidad\n",
    "\n",
    "epochs = 20\n",
    "progress_per = 2\n",
    "\n",
    "# Obtenemos los caminos\n",
    "# walks = random_walk(G, walk_length=walk_length, num_walks_per_node=num_walks_per_node)\n",
    "walks = node2vec(G, walk_length=walk_length, num_walks_per_node=num_walks_per_node, p=p, q=q)\n",
    "\n",
    "encoder = NodeEncoder(\n",
    "    vector_size = vector_size, # Tamaño del vector de embeddings\n",
    "    window = window, # Tamaño de la ventana\n",
    "    sg = sg, # Usamos Skip-Gram\n",
    "    hs = hs, # Usamos Negative Sampling\n",
    "    negative = negative, # Número de muestras negativas\n",
    "    alpha = alpha, # Tasa de aprendizaje\n",
    "    min_alpha = min_alpha, # Tasa de aprendizaje mínima\n",
    "    min_count = min_count, # Número mínimo de veces que una palabra debe aparecer en el corpus\n",
    "    seed = seed # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Codificamos los nodos\n",
    "embeddings = encoder.fit_transform(walks, epochs=epochs, progress_per=progress_per) # Generamos los embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función obtendrá la representación para cada una de las aristas aplicando una operación sobre los embeddings de los nodos conectados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_embedding(G, node_embeddings, op='mean'):\n",
    "    edge_embeddings = {}\n",
    "    \n",
    "    for u, v in G.edges():\n",
    "        u_embedding = node_embeddings.get(str(u), None)\n",
    "        v_embedding = node_embeddings.get(str(v), None)\n",
    "        \n",
    "        if u_embedding is not None and v_embedding is not None:\n",
    "            match op:\n",
    "                case 'mean':\n",
    "                    edge_embedding = (u_embedding + v_embedding) / 2\n",
    "                case 'hadamard':\n",
    "                    edge_embedding = u_embedding * v_embedding\n",
    "                case 'sum':\n",
    "                    edge_embedding = u_embedding + v_embedding\n",
    "                case 'sub':\n",
    "                    edge_embedding = u_embedding - v_embedding\n",
    "                case 'concat':\n",
    "                    edge_embedding = np.concatenate([u_embedding, v_embedding])\n",
    "                case _:\n",
    "                    raise ValueError(f'Operación no soportada: {op}')\n",
    "            \n",
    "            edge_embeddings[f\"{(u, v)}\"] = edge_embedding\n",
    "    \n",
    "    return edge_embeddings\n",
    "\n",
    "edge_embeddings = edge_embedding(G, node_embeddings=embeddings, op='sum')\n",
    "\n",
    "pd.DataFrame(edge_embeddings).T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinamos las representaciones de las aristas y la etiqueta en un mismo DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(pd.DataFrame(edge_embeddings).T, pd.Series({f\"{k}\": v for k, v in edge_labels.items()}, name='label'), left_index=True, right_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera, hacemos splits y entrenamos como si se tratara de clasificación de datos tabulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = generate_dataset(df, eval_size=0.2) # Generamos el dataset\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, # Número de árboles\n",
    "    max_depth=5, # Profundidad máxima\n",
    "    random_state=42 # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "classifier = train_classifier(X_train, y_train, X_eval, y_eval) # Entrenamos el clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line graph\n",
    "\n",
    "Alternativamente podemos crear un grafo en el que cada nodo represente una arista del grafo original. Dos nodos estarán conectados si entre las aristas compartían un nodo en común."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_G = nx.line_graph(G)\n",
    "line_G.labels = edge_labels # Añadimos las etiquetas de las aristas con el anterior criterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = GraphVisualizer(G)\n",
    "visualizer.visualize(edge_labels=edge_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El grafo resultante _lineG_ tendrá tantos nodos como el original aristas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_G_visualizer = GraphVisualizer(line_G)\n",
    "line_G_visualizer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos nuestra pipeline para node encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HIPERPARÁMETROS ###\n",
    "\n",
    "# Estrategia de camino\n",
    "walk_length = 5\n",
    "num_walks_per_node = 5\n",
    "p = 1\n",
    "q = 1\n",
    "\n",
    "# Encoder\n",
    "vector_size = 10 # Tamaño del vector de embeddings\n",
    "window = 4 # Tamaño de la ventana\n",
    "sg = 1 # Usamos Skip-Gram\n",
    "hs = 0 # Usamos Negative Sampling\n",
    "negative = 10 # Número de muestras negativas\n",
    "alpha = 0.03 # Tasa de aprendizaje\n",
    "min_alpha = 0.0001 # Tasa de aprendizaje mínima\n",
    "min_count = 1 # Número mínimo de veces que una palabra debe aparecer en el corpus\n",
    "seed = 42 # Semilla para reproducibilidad\n",
    "\n",
    "epochs = 20\n",
    "progress_per = 2\n",
    "\n",
    "# Obtenemos los caminos\n",
    "# walks = random_walk(line_G, walk_length=walk_length, num_walks_per_node=num_walks_per_node)\n",
    "walks = node2vec(line_G, walk_length=walk_length, num_walks_per_node=num_walks_per_node, p=p, q=q)\n",
    "\n",
    "encoder = NodeEncoder(\n",
    "    vector_size = vector_size, # Tamaño del vector de embeddings\n",
    "    window = window, # Tamaño de la ventana\n",
    "    sg = sg, # Usamos Skip-Gram\n",
    "    hs = hs, # Usamos Negative Sampling\n",
    "    negative = negative, # Número de muestras negativas\n",
    "    alpha = alpha, # Tasa de aprendizaje\n",
    "    min_alpha = min_alpha, # Tasa de aprendizaje mínima\n",
    "    min_count = min_count, # Número mínimo de veces que una palabra debe aparecer en el corpus\n",
    "    seed = seed # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Codificamos los nodos\n",
    "embeddings = encoder.fit_transform(walks, epochs=epochs, progress_per=progress_per) # Generamos los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(pd.DataFrame(edge_embeddings).T, pd.Series({f\"{k}\": v for k, v in edge_labels.items()}, name='label'), left_index=True, right_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = generate_dataset(df, eval_size=0.2) # Generamos el dataset\n",
    "classifier = RandomForestClassifier(\n",
    "    n_estimators=100, # Número de árboles\n",
    "    max_depth=5, # Profundidad máxima\n",
    "    random_state=42 # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "classifier = train_classifier(X_train, y_train, X_eval, y_eval) # Entrenamos el clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings para grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio práctico vamos a trabajar con el dataset **MUTAG**.\n",
    "\n",
    "Se trata de un dataset de clasificación de grafos. Cada grafo representa una molécula, donde los nodos corresponden a átomos y aristas representan enlaces químicos. Se utiliza para predecir si una molécula tiene propiedades mutagénicas o no. Dispone de 188 grafos y es un estándar para evaluar modelos en tareas de clasificación de grafos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mutag(n=None, centinel_node=False):\n",
    "    dataset = TUDataset(root='data/TUDataset', name='MUTAG') # Cargar el dataset MUTAG\n",
    "    dataset = dataset[:n] if n else dataset # Limitar el número de grafos si es necesario\n",
    "    \n",
    "    # Crear un grafo de NetworkX vacío\n",
    "    combined_graph = nx.Graph()\n",
    "    graphs_info = {}\n",
    "\n",
    "    # Iterar sobre todos los grafos del dataset y combinarlos\n",
    "    for graph_idx, graph in enumerate(dataset):\n",
    "        # Convertir cada grafo de PyTorch Geometric a NetworkX\n",
    "        nx_graph = to_networkx(graph, to_undirected=True)\n",
    "\n",
    "        # Renombrar los nodos para evitar colisiones\n",
    "        nx_graph = nx.relabel_nodes(nx_graph, lambda x: x + len(combined_graph))\n",
    "\n",
    "        # Añadir un nodo centinela al grafo\n",
    "        if centinel_node:\n",
    "            nx_graph.add_node(f'c_{graph_idx}')\n",
    "            for node in nx_graph.nodes:\n",
    "                nx_graph.add_edge(f'c_{graph_idx}', node)\n",
    "\n",
    "        # Les añadimos a cada nodo el grafo de pertenencia como atributo\n",
    "        nx.set_node_attributes(nx_graph, {node: graph_idx for node in nx_graph.nodes}, 'graph_id')\n",
    "        \n",
    "        graphs_info[graph_idx] = {\n",
    "            'node_ids': list(nx_graph.nodes),\n",
    "            'label': graph.y.numpy().item(),\n",
    "        }\n",
    "\n",
    "        # Agregar al grafo combinado\n",
    "        combined_graph = nx.compose(combined_graph, nx_graph)\n",
    "    \n",
    "    return combined_graph, graphs_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como se ven los n primeros grafos\n",
    "\n",
    "*NOTA*: Por como se visualizan con networkX puede parecer que a veces hay dos grafos que están conexos, pero no lo están."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "\n",
    "graph, graphs_info = load_mutag(n=n)\n",
    "visualizer = GraphVisualizer(graph)\n",
    "visualizer.visualize(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_info # Información de cada sub-grafo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como se recorren con random walk o node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 'node2vec'\n",
    "start_node = 17\n",
    "walk_length = 10\n",
    "p = 2\n",
    "q = 0.5\n",
    "\n",
    "visualizer = GraphVisualizer(graph)\n",
    "ani = visualizer.visualize_walk(R=R, start_node=start_node, walk_length=walk_length, p=p, q=q)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoque trivial por agregación\n",
    "\n",
    "Computaremos los embeddings de todos los nodos de cada sub-grafo y los agregaremos según cierta operación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora carguémoslo entero\n",
    "\n",
    "*IMPORTANTE*: si la persona leyendo esto lo está ejecutando en local y tiene cierto cariño a su ordenador, recomiendo no tratar de visualizar el grafo entero :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, graphs_info = load_mutag() # Cargamos el dataset MUTAG entero\n",
    "\n",
    "# Información sobre el grafo combinado\n",
    "print(f\"Número total de nodos: {graph.number_of_nodes()}\")\n",
    "print(f\"Número total de aristas: {graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computamos node2vec sobre el grafo completo (todos los sub-grafos inconexos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agreguemos los embeddings de cada sub-grafo. Para ello se requiere crear una función _compute\\_graph\\_embedding()_ que recibirá el parámetro _op_ que podrá recibir tres valores:\n",
    "\n",
    "- 'mean': realizará la media\n",
    "- 'sum': realizará la suma\n",
    "- 'hadamard': realizará la multiplicación de Hadamard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos los embeddings de cada grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinamos los embeddings y las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A entrenar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregando un Nodo Centinela\n",
    "\n",
    "Recordemos que un nodo centinela es un nodo _virtual_ que se conecta a todos los nodos de cada sub-grafo. Usaremos el embedding de ese nodo como una representación de todo el sub-grafo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con un simple parámetro en la función _load_mutag()_ se añade un nodo centinela a cada sub-grafo con el id \"c_{graph_idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, graphs_info = load_mutag(centinel_node=True) # Cargamos el dataset MUTAG entero con un nodo centinela en cada subgrafo\n",
    "\n",
    "graphs_info[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y aquí no hay nada que agregar, directamente computamos node2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos los embeddings de cada centinela como las _features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y extraemos las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinamos embeddings y etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A entrenar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
